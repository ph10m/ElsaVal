{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init vader\n",
      "Init DistilBERT SST-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09668ef3428d4cc7b6cf508a084a2625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from SentimentClassifier import TwoStepSentimentClassifier\n",
    "classifier = TwoStepSentimentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate random seed\n",
    "np.random.seed(4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"STEP 1 2018-2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"$T$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment(row):\n",
    "    text = row[\"text\"]\n",
    "    restored_text = text.replace(mask, row[\"name\"])\n",
    "    sent = classifier.label_and_confidence(restored_text)\n",
    "    row[\"sentiment\"] = sent.label\n",
    "    row[\"sentiment_score\"] = sent.confidence\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(root, folder):\n",
    "    path = os.path.join(root, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Tollef\\\\Documents\\\\GitHub\\\\masterNEW\\\\REPO\\\\strise\\\\STEP 2 2018-2019'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_path = os.path.join(os.getcwd(), \"STEP 2 2018-2019\")\n",
    "store_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader threshold 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 1 2018-2019\\business_entities.csv\n",
      "(39875, 6)\n",
      "drop dups\n",
      "(10108, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 2/10108 [00:00<09:20, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping NaN\n",
      "(10108, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10108/10108 [09:05<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing in C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 2 2018-2019\\business_entities.csv\n",
      "reading C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 1 2018-2019\\politics_entities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/10708 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41211, 6)\n",
      "drop dups\n",
      "(10708, 6)\n",
      "dropping NaN\n",
      "(10708, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10708/10708 [09:26<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing in C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 2 2018-2019\\politics_entities.csv\n",
      "reading C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 1 2018-2019\\sports_entities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 6/10655 [00:00<03:45, 47.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40399, 6)\n",
      "drop dups\n",
      "(10655, 6)\n",
      "dropping NaN\n",
      "(10655, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10655/10655 [10:58<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing in C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 2 2018-2019\\sports_entities.csv\n",
      "reading C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 1 2018-2019\\tech_entities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                          | 0/9263 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36090, 6)\n",
      "drop dups\n",
      "(9263, 6)\n",
      "dropping NaN\n",
      "(9263, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 9263/9263 [08:39<00:00, 17.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing in C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 2 2018-2019\\tech_entities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_topics_entity_dfs = []\n",
    "all_topics_texts_dfs = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(data_path):\n",
    "    folder_name = os.path.split(subdir)[-1] if not dirs else None\n",
    "    if not folder_name:\n",
    "        continue\n",
    "        \n",
    "    topic_files = [f for f in files if '_' in f]\n",
    "    for csv in topic_files:\n",
    "        csv_path = os.path.join(subdir, csv)\n",
    "        print(\"reading\", csv_path)\n",
    "\n",
    "        ent_df = pd.read_csv(csv_path)\n",
    "        print(ent_df.shape)\n",
    "        \n",
    "        ent_df.drop_duplicates(subset=\"text\", inplace=True)\n",
    "        print(\"drop dups\")\n",
    "        print(ent_df.shape)\n",
    "        \n",
    "        ent_df = ent_df.dropna()\n",
    "        print(\"dropping NaN\")\n",
    "        print(ent_df.shape)\n",
    "        \n",
    "        ent_df[\"sentiment\"] = np.nan\n",
    "        ent_df[\"sentiment_score\"] = np.nan\n",
    "        \n",
    "        # use tqdm's progress_apply instead of apply\n",
    "        ent_df = ent_df.progress_apply(lambda row: apply_sentiment(row), axis=1)\n",
    "        \n",
    "        all_topics_entity_dfs.append(ent_df)\n",
    "        \n",
    "        new_path = os.path.join(store_path, csv)\n",
    "        print(\"storing in\", new_path)\n",
    "        ent_df.to_csv(new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, merge all the entities and store it in a singular csv\n",
    "final_entity_df = pd.concat(all_topics_entity_dfs)\n",
    "entity_path = os.path.join(store_path, \"entities.csv\")\n",
    "final_entity_df.to_csv(entity_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_topics_entity_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = None\n",
    "big = None\n",
    "for subdir, dirs, files in os.walk(data_path):\n",
    "    folder_name = os.path.split(subdir)[-1] if not dirs else None\n",
    "    if not folder_name:\n",
    "        continue\n",
    "        \n",
    "    topic_files = [f for f in files if '_' in f]\n",
    "    for csv in topic_files:\n",
    "        csv_path = os.path.join(subdir, csv)\n",
    "        if \"sport\" not in csv:\n",
    "            continue\n",
    "        print(\"reading\", csv_path)\n",
    "\n",
    "        ent_df = pd.read_csv(csv_path)\n",
    "        print(ent_df.shape)\n",
    "        big = ent_df\n",
    "        \n",
    "        print(\"exposing dplicates...\")\n",
    "        dups = ent_df.duplicated(subset=\"text\", keep=\"first\")\n",
    "        break\n",
    "        \n",
    "        ent_df.drop_duplicates(subset=\"text\", inplace=True)\n",
    "        print(\"drop dups\")\n",
    "        print(ent_df.shape)\n",
    "        \n",
    "        ent_df = ent_df.dropna()\n",
    "        print(\"dropping NaN\")\n",
    "        print(ent_df.shape)\n",
    "        \n",
    "        break\n",
    "    break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duped = big[big.duplicated(subset=[\"text\", \"name\"])]\n",
    "duped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtexts = duped.text\n",
    "len(dtexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in dtexts[0:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = big.reset_index().drop_duplicates(big.index.text)\n",
    "cleaned.set_index(big.index.names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big.reset_index().drop_duplicates(subset=\"references\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = big[0:100]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop_duplicates(subset=\"text\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
