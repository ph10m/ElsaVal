{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting distant supervision labeled targeted sentiment with coreference\n",
    "Functionality:\n",
    "- Load in a sentiment annotated dataset\n",
    "- Use the texts as basis for coreference annotation\n",
    "    - Combine texts with same target\n",
    "- Compute coreference\n",
    "- For each reference, replace the reference with the root entity $T$\n",
    "- Store a new dataset with the updated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\distant_supervision_train.seg C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\distant_supervision_test.seg\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\augmented_distant_supervision_train.seg C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\augmented_distant_supervision_test.seg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"STEP 3 - ABSA-format\")\n",
    "\n",
    "def get_filename(f):\n",
    "    return \"distant_supervision_{}.seg\".format(f)\n",
    "\n",
    "def create_filename(f):\n",
    "    return \"augmented_distant_supervision_{}.seg\".format(f)\n",
    "\n",
    "train = os.path.join(dataset_path, get_filename(\"train\"))\n",
    "test = os.path.join(dataset_path, get_filename(\"test\"))\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), \"STEP 4 - Augmented ABSA\")\n",
    "train_out = os.path.join(out_path, create_filename(\"train\"))\n",
    "test_out = os.path.join(out_path, create_filename(\"test\"))\n",
    "\n",
    "print(train, test)\n",
    "print(train_out, test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy model...\n",
      "Added neuralcoref to pipeline!\n"
     ]
    }
   ],
   "source": [
    "# init neuralcoref class\n",
    "from tollef_coref import Coref\n",
    "\n",
    "params = {\n",
    "    \"greed\": 0.53,\n",
    "    \"max_dist\": 50,\n",
    "    \"max_dist_match\": 500\n",
    "}\n",
    "coref = Coref(params, spacy_size=\"md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The food is uniformly exceptional , with a very capable kitchen which will proudly whip up whatever you feel like eating , whether it 's on the menu or not ."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"The food is uniformly exceptional , with a very capable kitchen which will proudly whip up whatever you feel like eating , whether it 's on the menu or not .\"\n",
    "coref.add_doc(txt)\n",
    "coref.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = \"$T$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(t):\n",
    "    t = re.sub(r\"(\\n+)(?=[A-Z])\", \" \", t)  # replace consecutive newlines connected to words\n",
    "    t = re.sub(r\"(\\n+)\", \" \", t)  # replace unconnected newlines\n",
    "    t = t.replace(\"-LRB-\", \"\")\n",
    "    t = t.replace(\"-RRB-\", \"\")\n",
    "    \n",
    "    # finally, make sure it is proper alphanumeric text\n",
    "    #pattern = re.compile('[\\W_]+', re.UNICODE)\n",
    "    #t = pattern.sub(' ', t)\n",
    "    return t.strip()  # aviod trailing space or newlines\n",
    "\n",
    "def valid_referential(token):\n",
    "    # \"both\" occurs in many contexts...\n",
    "    # it is only valid when it is avoiding the following:\n",
    "    # POS: DET\n",
    "    # DEPENDENCY: preconj\n",
    "    POS = token.pos_\n",
    "    DEP = token.dep_\n",
    "    valid_determiner = POS != \"DEP\" and DEP != \"preconj\"\n",
    "    \n",
    "    isvalid = valid_determiner\n",
    "    \n",
    "    return isvalid\n",
    "\n",
    "def print_pos(token):\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, \n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(f, text, tar, sent):\n",
    "    f.write(text)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(tar)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(sent)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "def parse_file(infile, outfile):\n",
    "    with open(outfile, 'w', encoding=\"utf8\") as outdata:\n",
    "        with open(infile, 'r', encoding=\"utf8\") as indata:\n",
    "            lines = indata.readlines()\n",
    "            step = 3\n",
    "            for i in range(0, len(lines), step):\n",
    "                text = lines[i].strip()\n",
    "                tar = lines[i+1].strip()\n",
    "                sent = lines[i+2].strip()\n",
    "                \n",
    "                write(outdata, text, tar, sent)\n",
    "\n",
    "                # format it before processing for coreference\n",
    "                text = clean(text)\n",
    "                # write the original cleaned data as well, duplicating it\n",
    "                # comment this step if it doesn't work.\n",
    "                # write(outdata, text, tar, sent)\n",
    "\n",
    "                tar_index = text.index(MASK)\n",
    "            \n",
    "                tar_span = (tar_index, tar_index + len(tar))\n",
    "\n",
    "                # unmask the target $T$ with the actual word before updating with coreference\n",
    "                text = text.replace(MASK, tar)\n",
    "                coref.add_doc(text)\n",
    "                \n",
    "                if coref.cluster_resolved and coref.clusters():\n",
    "                    for cluster in coref.clusters():\n",
    "                        # the real target is found as the root of the cluster\n",
    "                        if tar in cluster.main.text: \n",
    "                            # use this if the root should be included\n",
    "                            mentions = cluster.mentions  \n",
    "                            for mention in mentions:\n",
    "                                valid = False\n",
    "                                # set a threshold for the mention span length\n",
    "                                if len(mention) < 10:  \n",
    "                                    valid = any([valid_referential(ref) for ref in mention])\n",
    "                                if valid:\n",
    "                                    start, end = mention.start, mention.end\n",
    "                                    tokens = coref.tokens()\n",
    "                                    for _ in range(end-start):\n",
    "                                        # for each word in the new mention, remove it from the list\n",
    "                                        tokens.pop(start)\n",
    "                                    tokens.insert(start, MASK)\n",
    "                                    # format it back as a string\n",
    "                                    coref_text = ' '.join(tokens).strip()\n",
    "                                    if coref_text != lines[i].strip():\n",
    "                                        USE_MENTION = False\n",
    "                                        if USE_MENTION:\n",
    "                                            target = mention.text\n",
    "                                        else:\n",
    "                                            target = tar\n",
    "                                        write(outdata, coref_text, target, sent)\n",
    "                                        # using e.g. \"it\" instead of target\n",
    "                                        # write(outdata, coref_text,  mention.text, sent)  \n",
    "                                    #added_sents.append([coref_text, tar, sent])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate, save copies of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\business_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_business_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\business_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_business_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\distant_all_data.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_distant_all_data.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\entities.csv_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_entities.csv_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\entities.csv_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_entities.csv_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\politics_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_politics_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\politics_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_politics_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\sports_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_sports_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\sports_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_sports_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\tech_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_tech_distant_test.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 3 - ABSA-format\\tech_distant_train.litesent\n",
      "C:\\Users\\Tollef\\Documents\\GitHub\\masterNEW\\REPO\\strise\\STEP 4 - Augmented ABSA\\coref_tech_distant_train.litesent\n"
     ]
    }
   ],
   "source": [
    "for topicfile in os.listdir(dataset_path):\n",
    "    topic_path = os.path.join(dataset_path, topicfile)\n",
    "    print(topic_path)\n",
    "    save_name = \"coref_\" + topicfile\n",
    "    save_path = os.path.join(out_path, save_name)\n",
    "    print(save_path)\n",
    "    parse_file(topic_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(train, train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(test, test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING, remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(infile):\n",
    "    with open(infile, 'r', encoding=\"utf8\") as indata:\n",
    "        lines = indata.readlines()\n",
    "        step = 3\n",
    "        for i in range(0, len(lines), step):\n",
    "            text = lines[i].strip()\n",
    "            tar = lines[i+1].strip()\n",
    "            sent = lines[i+2].strip()\n",
    "\n",
    "            # format it before processing for coreference\n",
    "            text = clean(text)\n",
    "\n",
    "            tar_index = text.index(MASK)\n",
    "\n",
    "            tar_span = (tar_index, tar_index + len(tar))\n",
    "\n",
    "            # unmask the target $T$ with the actual word before updating with coreference\n",
    "            text = text.replace(MASK, tar)\n",
    "            coref.add_doc(text)\n",
    "\n",
    "            if coref.cluster_resolved and coref.clusters():\n",
    "                for cluster in coref.clusters():\n",
    "                    # the real target is found as the root of the cluster\n",
    "                    if tar in cluster.main.text: \n",
    "                        # use this if the root should be included\n",
    "                        mentions = cluster.mentions  \n",
    "                        for mention in mentions:\n",
    "                            print(mention)\n",
    "                            print(\"-->\", mention.text)\n",
    "                            valid = False\n",
    "                            # set a threshold for the mention span length\n",
    "                            if len(mention) < 10:  \n",
    "                                valid = any([valid_referential(ref) for ref in mention])\n",
    "                            if valid:\n",
    "                                start, end = mention.start, mention.end\n",
    "                                tokens = coref.tokens()\n",
    "                                for _ in range(end-start):\n",
    "                                    # for each word in the new mention, remove it from the list\n",
    "                                    tokens.pop(start)\n",
    "                                tokens.insert(start, MASK)\n",
    "                                # format it back as a string\n",
    "                                coref_text = ' '.join(tokens).strip()\n",
    "                                if coref_text != lines[i].strip():\n",
    "                                    USE_MENTION = True\n",
    "                                    if USE_MENTION:\n",
    "                                        target = mention.target\n",
    "                                    else:\n",
    "                                        target = tar\n",
    "\n",
    "                                    print(\"writing:\\n{}\\n---{}\".format(coref_text, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corporate leet\n",
      "--> the corporate leet\n",
      "writing:\n",
      "Before his landslide election in July 2018 , Lopez Obrador provoked anxiety among $T$ as he disparagingly called it , dismissing big - business people as traffickers of influence .\n",
      "---leet\n",
      "it\n",
      "--> it\n",
      "writing:\n",
      "Before his landslide election in July 2018 , Lopez Obrador provoked anxiety among the corporate leet as he disparagingly called $T$ , dismissing big - business people as traffickers of influence .\n",
      "---leet\n",
      "The Ashes series\n",
      "--> The Ashes series\n",
      "writing:\n",
      "The 76-year - old , in a piece for ESPNcricinfo , claimed that although the recently concluded $T$ breathed life into the longer format of the game , it faces serious challenges up ahead .\n",
      "---The Ashes\n",
      "it\n",
      "--> it\n",
      "writing:\n",
      "The 76-year - old , in a piece for ESPNcricinfo , claimed that although the recently concluded The Ashes series breathed life into the longer format of the game , $T$ faces serious challenges up ahead .\n",
      "---The Ashes\n",
      "a 3-year term-based subscription\n",
      "--> a 3-year term-based subscription\n",
      "writing:\n",
      "Assuming equivalent pricing , $T$ would not recognize the same amount of revenue until it renews after year 3 In short , Nutanix has moved 77 of its business to subscription while not ( yet ) realizing the benefits .\n",
      "---3\n",
      "it\n",
      "--> it\n",
      "writing:\n",
      "Assuming equivalent pricing , a 3-year term - based subscription would not recognize the same amount of revenue until $T$ renews after year 3 In short , Nutanix has moved 77 of its business to subscription while not ( yet ) realizing the benefits .\n",
      "---3\n",
      "President Cyril Matamela Cyril Ramaphosa\n",
      "--> President Cyril Matamela Cyril Ramaphosa\n",
      "writing:\n",
      "When $T$ announced a national lockdown last week , he also announced provisions of a combined R2 billion from the family of Remgro billionaire Johann Rupert and De Beers billionaire Nicky Oppenheimer .\n",
      "---Matamela Cyril Ramaphosa\n",
      "he\n",
      "--> he\n",
      "writing:\n",
      "When President Cyril Matamela Cyril Ramaphosa announced a national lockdown last week , $T$ also announced provisions of a combined R2 billion from the family of Remgro billionaire Johann Rupert and De Beers billionaire Nicky Oppenheimer .\n",
      "---Matamela Cyril Ramaphosa\n",
      "Congressman Alan Lowenthal Canada\n",
      "--> Congressman Alan Lowenthal Canada\n",
      "writing:\n",
      "$T$ joined with 10 of his House colleagues to call on the Ambassador .\n",
      "---Canada\n",
      "his\n",
      "--> his\n",
      "writing:\n",
      "Congressman Alan Lowenthal Canada joined with 10 of $T$ House colleagues to call on the Ambassador .\n",
      "---Canada\n",
      "Wednesday champion a coalition deal and clearly we'll offer the European Greens the opportunity to join\n",
      "--> Wednesday champion a coalition deal and clearly we'll offer the European Greens the opportunity to join\n",
      "we\n",
      "--> we\n",
      "writing:\n",
      "Wednesday champion a coalition deal and clearly $T$ 'll offer the European Greens the opportunity to join , he told French radio this week .\n",
      "---Wednesday\n",
      "he\n",
      "--> he\n",
      "writing:\n",
      "Wednesday champion a coalition deal and clearly we 'll offer the European Greens the opportunity to join , $T$ told French radio this week .\n",
      "---Wednesday\n",
      "we take just three of those 2,374 cases, involving Perrigo, Analog Devices and Takeda Pharmaceutical Company\n",
      "--> we take just three of those 2,374 cases, involving Perrigo, Analog Devices and Takeda Pharmaceutical Company\n",
      "them\n",
      "--> them\n",
      "writing:\n",
      "If we take just three of those 2,374 cases , involving Perrigo , Analog Devices and Takeda Pharmaceutical Company the three companies between $T$ received upward reassessments of over 2 billion .\n",
      "---Takeda Pharmaceutical Company\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c007e298850b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparse_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-ca7e2d4eca07>\u001b[0m in \u001b[0;36mparse_file\u001b[1;34m(infile)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# unmask the target $T$ with the actual word before updating with coreference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMASK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mcoref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcoref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_resolved\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcoref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\documents\\github\\masternew\\repo\\modules\\tollef_coref\\__init__.py\u001b[0m in \u001b[0;36madd_doc\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcluster_resolved\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Callable\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\neural\\_classes\\relu.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input__BI)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"nI\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moutput__BO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moutput__BO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__BO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput__BO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Callable\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\thinc\\neural\\_classes\\affine.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input__BI)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"nI\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput__BI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parse_file(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
